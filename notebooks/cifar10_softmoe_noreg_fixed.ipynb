{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-10 SoftMoE (no regularization)\n",
        "\n",
        "Setup, train, and visualize metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: clone repo and install requirements\n",
        "%cd /content  # go back to root\n",
        "!git clone https://github.com/moe-project-uu/mixture-of-experts-project.git || true\n",
        "%cd mixture-of-experts-project\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "# Use Colab's preinstalled torch; install package in editable mode\n",
        "%pip install -U pip\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train via CLI so argparse is used\n",
        "%cd /content/mixture-of-experts-project\n",
        "\n",
        "# config settings\n",
        "FF_LAYER = \"SoftMoE\"   # \"Dense\" or \"SoftMoE\"\n",
        "EPOCHS   = 100\n",
        "NUM_EXPERTS = 4        # ignored for Dense\n",
        "\n",
        "if FF_LAYER == \"Dense\":\n",
        "    !python scripts/train_cifar10.py --FF_layer Dense --epochs $EPOCHS\n",
        "else:\n",
        "    !python scripts/train_cifar10.py --FF_layer SoftMoE --epochs $EPOCHS --num_experts $NUM_EXPERTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metrics.pt and plot loss/accuracy\n",
        "import os, torch, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Must match the run config\n",
        "FF_LAYER = FF_LAYER\n",
        "EPOCHS   = EPOCHS\n",
        "NUM_EXPERTS = NUM_EXPERTS\n",
        "\n",
        "run_tag = f\"E{EPOCHS}\" if FF_LAYER == \"Dense\" else f\"E{EPOCHS}-X{NUM_EXPERTS}\"\n",
        "ckpt_dir = os.path.join(\"/content/mixture-of-experts-project\", \"checkpoints\", FF_LAYER, run_tag)\n",
        "metrics_path = os.path.join(ckpt_dir, \"metrics.pt\")\n",
        "assert os.path.exists(metrics_path), f\"metrics.pt not found at {metrics_path}\"\n",
        "\n",
        "# load hist dict from metrics.pt\n",
        "hist = torch.load(metrics_path, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "train_loss = np.array(hist[\"train_loss\"])\n",
        "train_acc  = np.array(hist[\"train_acc\"])\n",
        "val_loss   = np.array(hist[\"val_loss\"])\n",
        "val_acc    = np.array(hist[\"val_acc\"])\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_loss, label=\"train\")\n",
        "plt.plot(val_loss,   label=\"val\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(f\"{FF_LAYER} — Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_acc, label=\"train\")\n",
        "plt.plot(val_acc,   label=\"val\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(f\"{FF_LAYER} — Accuracy\"); plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expert Utilization\n",
        "from moe.utils.helpers import plot_expert_utilization\n",
        "plot_expert_utilization(hist, ff_layer=FF_LAYER, epochs_to_bar=(0, 50, 99))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gating Entropy\n",
        "from moe.utils.helpers import plot_gating_entropy\n",
        "plot_gating_entropy(hist, ff_layer=FF_LAYER)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
