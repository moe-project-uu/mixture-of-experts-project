{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f395735f",
   "metadata": {},
   "source": [
    "# CIFAR-10 Baseline (Colab)\n",
    "\n",
    "This notebook loads the repo, installs deps, optionally runs the trainer, and visualizes saved metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3809b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Setup: clone repo and install requirements\n",
    "%cd /content  # go back to root\n",
    "!rm -rf mixture-of-experts-project\n",
    "!git clone https://github.com/moe-project-uu/mixture-of-experts-project.git || true\n",
    "%cd mixture-of-experts-project\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# 2) Use Colab’s preinstalled torch to avoid CUDA mismatches\n",
    "#    (so don't reinstall torch/torchvision). Install your package + other deps.\n",
    "%pip install -U pip\n",
    "%pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL2: run training and capture history\n",
    "import sys, os\n",
    "proj = \"/content/mixture-of-experts-project\"\n",
    "sys.path.insert(0, proj)                    # for scripts/\n",
    "sys.path.insert(0, os.path.join(proj, \"src\"))  # ← for src/moe package\n",
    "\n",
    "from scripts.train_cifar10 import parser, main  # now works\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.FF_layer = \"SoftMoE\"   # or \"Dense\"\n",
    "args.epochs   = 100\n",
    "args.num_experts = 4        # ignored for Dense\n",
    "\n",
    "hist = main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b91bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL3: Loss/Accuracy curves from `hist`\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = np.array(hist[\"train_loss\"])\n",
    "train_acc  = np.array(hist[\"train_acc\"])\n",
    "val_loss   = np.array(hist[\"val_loss\"])\n",
    "val_acc    = np.array(hist[\"val_acc\"])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_loss, label=\"train\")\n",
    "plt.plot(val_loss,   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(f'{args.FF_layer} — Loss'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_acc, label=\"train\")\n",
    "plt.plot(val_acc,   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(f'{args.FF_layer} — Accuracy'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986277f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL3: Loss/Accuracy curves from `hist`\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = np.array(hist[\"train_loss\"])\n",
    "train_acc  = np.array(hist[\"train_acc\"])\n",
    "val_loss   = np.array(hist[\"val_loss\"])\n",
    "val_acc    = np.array(hist[\"val_acc\"])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_loss, label=\"train\")\n",
    "plt.plot(val_loss,   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(f'{args.FF_layer} — Loss'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_acc, label=\"train\")\n",
    "plt.plot(val_acc,   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(f'{args.FF_layer} — Accuracy'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL4: SoftMoE — Expert utilization over epochs + per-expert bars at a few epochs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if args.FF_layer != \"SoftMoE\":\n",
    "    print(\"Utilization plots are only available for SoftMoE. Re-run with args.FF_layer='SoftMoE'.\")\n",
    "else:\n",
    "    util_list = hist.get(\"util_per_epoch\", [])\n",
    "    assert len(util_list) > 0, \"No utilization recorded — ensure you trained with SoftMoE.\"\n",
    "\n",
    "    util = np.stack(util_list, axis=0)  # shape: (num_epochs, num_experts)\n",
    "\n",
    "    # (A) line plot: one curve per expert across epochs\n",
    "    plt.figure(figsize=(7,4))\n",
    "    for i in range(util.shape[1]):\n",
    "        plt.plot(util[:, i], label=f\"expert {i}\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"mean p_i\")\n",
    "    plt.title(\"Expert Utilization over epochs\")\n",
    "    plt.legend(ncol=2); plt.show()\n",
    "\n",
    "    # (B) one-bar-per-expert “histograms” at epochs ~0, 50, 100 (clamped to available range)\n",
    "    want = [0, 50, 100]\n",
    "    max_idx = util.shape[0] - 1\n",
    "    picked = [e for e in want if e <= max_idx]\n",
    "\n",
    "    for e in picked:\n",
    "        vals = util[e]  # length = num_experts\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(np.arange(len(vals)), vals)\n",
    "        plt.xticks(np.arange(len(vals)), [f\"E{i}\" for i in range(len(vals))])\n",
    "        plt.ylim(0, 1)\n",
    "        plt.ylabel(\"mean p_i\"); plt.xlabel(\"expert\")\n",
    "        plt.title(f\"Expert mean probabilities at epoch index {e} (0-based)\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL5: SoftMoE — Gating entropy over epochs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if args.FF_layer != \"SoftMoE\":\n",
    "    print(\"Entropy plot is only available for SoftMoE. Re-run with args.FF_layer='SoftMoE'.\")\n",
    "else:\n",
    "    H = np.array(hist.get(\"entropy_per_epoch\", []))\n",
    "    if H.size == 0:\n",
    "        print(\"No entropy recorded — make sure SoftMoE ran.\")\n",
    "    else:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(H)\n",
    "        plt.xlabel(\"epoch\"); plt.ylabel(\"entropy  H = -Σ p log p\")\n",
    "        plt.title(\"Gating Entropy over epochs\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95217b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
