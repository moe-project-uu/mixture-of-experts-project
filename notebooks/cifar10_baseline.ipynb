{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f395735f",
   "metadata": {},
   "source": [
    "# CIFAR-10 Baseline (Colab)\n",
    "\n",
    "This notebook loads the repo, installs deps, optionally runs the trainer, and visualizes saved metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3809b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Setup: clone repo and install requirements\n",
    "%cd /content  # go back to root\n",
    "!git clone https://github.com/moe-project-uu/mixture-of-experts-project.git || true\n",
    "%cd mixture-of-experts-project\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# 2) Use Colab’s preinstalled torch to avoid CUDA mismatches\n",
    "#    (so don't reinstall torch/torchvision). Install your package + other deps.\n",
    "%pip install -U pip\n",
    "%pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL2: Train via CLI so argparse is used\n",
    "%cd /content/mixture-of-experts-project\n",
    "\n",
    "# config settings\n",
    "FF_LAYER = \"SoftMoE\"   # \"Dense\" or \"SoftMoE\"\n",
    "EPOCHS   = 100\n",
    "NUM_EXPERTS = 4        # ignored for Dense\n",
    "\n",
    "if FF_LAYER == \"Dense\":\n",
    "    !python scripts/train_cifar10.py --FF_layer Dense --epochs {EPOCHS}\n",
    "else:\n",
    "    !python scripts/train_cifar10.py --FF_layer SoftMoE --epochs {EPOCHS} --num_experts {NUM_EXPERTS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b91bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL3: Load metrics.pt and plot loss/accuracy\n",
    "import os, torch, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FF_LAYER = \"SoftMoE\"   # must match CELL2\n",
    "EPOCHS   = 100\n",
    "NUM_EXPERTS = 4        # only used if not Dense\n",
    "\n",
    "run_tag = f\"E{EPOCHS}\" if FF_LAYER == \"Dense\" else f\"E{EPOCHS}-X{NUM_EXPERTS}\"\n",
    "ckpt_dir = os.path.join(\"/content/mixture-of-experts-project\", \"checkpoints\", FF_LAYER, run_tag)\n",
    "metrics_path = os.path.join(ckpt_dir, \"metrics.pt\")\n",
    "assert os.path.exists(metrics_path), f\"metrics.pt not found at {metrics_path}\"\n",
    "\n",
    "#load hist dict from metrics.pt\n",
    "hist = torch.load(metrics_path, map_location=\"cpu\")\n",
    "\n",
    "train_loss = np.array(hist[\"train_loss\"])\n",
    "train_acc  = np.array(hist[\"train_acc\"])\n",
    "val_loss   = np.array(hist[\"val_loss\"])\n",
    "val_acc    = np.array(hist[\"val_acc\"])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_loss, label=\"train\")\n",
    "plt.plot(val_loss,   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(f\"{FF_LAYER} — Loss\"); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_acc, label=\"train\")\n",
    "plt.plot(val_acc,   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(f\"{FF_LAYER} — Accuracy\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986277f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL4: SoftMoE — Expert Utilization (lines + per-epoch bars)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if FF_LAYER != \"SoftMoE\":\n",
    "    print(\"Utilization plots are only available for SoftMoE.\")\n",
    "else:\n",
    "    util_list = hist.get(\"util_per_epoch\", [])\n",
    "    if len(util_list) == 0:\n",
    "        print(\"No utilization recorded. Make sure you trained with SoftMoE.\")\n",
    "    else:\n",
    "        util = np.stack(util_list, axis=0)  # shape (num_epochs, num_experts)\n",
    "\n",
    "        # (A) Line plot: evolution over epochs\n",
    "        plt.figure(figsize=(7,4))\n",
    "        for i in range(util.shape[1]):\n",
    "            plt.plot(util[:, i], label=f\"expert {i}\")\n",
    "        plt.xlabel(\"epoch\"); plt.ylabel(\"mean p_i\")\n",
    "        plt.title(\"Expert Utilization over epochs\")\n",
    "        plt.legend(ncol=2); plt.show()\n",
    "\n",
    "        # (B) Bars at epochs 0, 50, 100 (0-based indices)\n",
    "        requested = [0, 50, 100]\n",
    "        max_idx = util.shape[0] - 1\n",
    "        picked = [e for e in requested if 0 <= e <= max_idx]\n",
    "\n",
    "        for e in picked:\n",
    "            vals = util[e]  # length = num_experts\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.bar(np.arange(len(vals)), vals)\n",
    "            plt.xticks(np.arange(len(vals)), [f\"E{i}\" for i in range(len(vals))])\n",
    "            plt.ylim(0, 1)\n",
    "            plt.ylabel(\"mean p_i\"); plt.xlabel(\"expert\")\n",
    "            plt.title(f\"Expert mean probabilities at epoch index {e} (0-based)\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL5: SoftMoE — Gating Entropy over epochs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if FF_LAYER != \"SoftMoE\":\n",
    "    print(\"Entropy plot is only available for SoftMoE.\")\n",
    "else:\n",
    "    H = np.array(hist.get(\"entropy_per_epoch\", []))\n",
    "    if H.size == 0:\n",
    "        print(\"No entropy recorded. Make sure you trained with SoftMoE.\")\n",
    "    else:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(H)\n",
    "        plt.xlabel(\"epoch\"); plt.ylabel(\"entropy  H = -Σ p log p\")\n",
    "        plt.title(\"Gating Entropy over epochs\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
